{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=pd.read_csv(\"CERNER_TWEET_DB/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usernameTweet</th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>nbr_retweet</th>\n",
       "      <th>nbr_favorite</th>\n",
       "      <th>nbr_reply</th>\n",
       "      <th>datetime</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>user_id</th>\n",
       "      <th>has_media</th>\n",
       "      <th>medias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MTomDem</td>\n",
       "      <td>1198415786420981760</td>\n",
       "      <td>The SC snakes are “Sporting Club” and a refere...</td>\n",
       "      <td>/MTomDem/status/1198415786420981760</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-24 07:09:23</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>48181411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MattKDubs</td>\n",
       "      <td>1198416444641464322</td>\n",
       "      <td>Cerner  snakes? Man, that's a deep take. Again...</td>\n",
       "      <td>/MattKDubs/status/1198416444641464322</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-24 07:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>974603336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>GevonnaWilliams</td>\n",
       "      <td>1198458541918568448</td>\n",
       "      <td>Great opportunity for  Cerner  HIM - Sr. Reven...</td>\n",
       "      <td>/GevonnaWilliams/status/1198458541918568448</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-24 09:59:17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2460446502</td>\n",
       "      <td>True</td>\n",
       "      <td>['https://t.co/TKKpbEOGnu']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GevonnaWilliams</td>\n",
       "      <td>1198458837042421760</td>\n",
       "      <td>Great opportunity for  Cerner  FirstNet Analys...</td>\n",
       "      <td>/GevonnaWilliams/status/1198458837042421760</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-24 10:00:27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2460446502</td>\n",
       "      <td>True</td>\n",
       "      <td>['https://t.co/VPdXC3H0b0']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GevonnaWilliams</td>\n",
       "      <td>1198458869699219456</td>\n",
       "      <td>Great opportunity for  Cerner  Physician Docum...</td>\n",
       "      <td>/GevonnaWilliams/status/1198458869699219456</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-24 10:00:35</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2460446502</td>\n",
       "      <td>True</td>\n",
       "      <td>['https://t.co/F4Cq1nnLRP']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     usernameTweet                   ID  \\\n",
       "0          MTomDem  1198415786420981760   \n",
       "1        MattKDubs  1198416444641464322   \n",
       "2  GevonnaWilliams  1198458541918568448   \n",
       "3  GevonnaWilliams  1198458837042421760   \n",
       "4  GevonnaWilliams  1198458869699219456   \n",
       "\n",
       "                                                text  \\\n",
       "0  The SC snakes are “Sporting Club” and a refere...   \n",
       "1  Cerner  snakes? Man, that's a deep take. Again...   \n",
       "2  Great opportunity for  Cerner  HIM - Sr. Reven...   \n",
       "3  Great opportunity for  Cerner  FirstNet Analys...   \n",
       "4  Great opportunity for  Cerner  Physician Docum...   \n",
       "\n",
       "                                           url  nbr_retweet  nbr_favorite  \\\n",
       "0          /MTomDem/status/1198415786420981760            0             2   \n",
       "1        /MattKDubs/status/1198416444641464322            0             2   \n",
       "2  /GevonnaWilliams/status/1198458541918568448            0             0   \n",
       "3  /GevonnaWilliams/status/1198458837042421760            0             0   \n",
       "4  /GevonnaWilliams/status/1198458869699219456            0             0   \n",
       "\n",
       "   nbr_reply             datetime  is_reply  is_retweet     user_id has_media  \\\n",
       "0          1  2019-11-24 07:09:23      True       False    48181411       NaN   \n",
       "1          1  2019-11-24 07:12:00      True       False   974603336       NaN   \n",
       "2          0  2019-11-24 09:59:17     False       False  2460446502      True   \n",
       "3          0  2019-11-24 10:00:27     False       False  2460446502      True   \n",
       "4          0  2019-11-24 10:00:35     False       False  2460446502      True   \n",
       "\n",
       "                        medias  \n",
       "0                          NaN  \n",
       "1                          NaN  \n",
       "2  ['https://t.co/TKKpbEOGnu']  \n",
       "3  ['https://t.co/VPdXC3H0b0']  \n",
       "4  ['https://t.co/F4Cq1nnLRP']  "
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us remove usermentions, hashtags and urls from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURL(text,replace_text=\"\"):\n",
    "    re_url=\"http(\\S+)*|\\S+\\.com\\S+|bit.ly\\S+|\\S+utm_source\\S+\" #\\S+ matched all non-whitespace characters\n",
    "    return re.sub(re_url,replace_text,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  '"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeURL(\"https://www. bizjournals.com/boston/news/20 19/11/25/health-it-companycerner-plans-layoffs-in-waltham.html?ana=TRUEANTHEMTWT_BO&taid=5ddbaeb0bd53880001eb8dab&utm_campaign=trueAnthem%3A+New+Content+%28Feed%29&utm_medium=trueAnthem&utm_source=twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUserMentions(text,replace_text=\"\"):\n",
    "    re_usermentions=\"^@\\w{1,15}|@\\w{1,15}\"\n",
    "    return re.sub(re_usermentions,replace_text,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cleaned_tweet']=tweets['text'].apply(lambda x:removeUserMentions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cleaned_tweet']=tweets['cleaned_tweet'].apply(lambda x:removeURL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeHashTag(text,replace_text=\"\"):\n",
    "    re_hashtag=\"#\\S+\"\n",
    "    return re.sub(re_hashtag,replace_text,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cleaned_tweet']=tweets['cleaned_tweet'].apply(lambda x:removeHashTag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['score']=tweets['cleaned_tweet'].apply(lambda x:TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['polarity']=tweets['score'].apply(lambda x:x.polarity)\n",
    "tweets['subjectivity']=tweets['score'].apply(lambda x:x.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSenti(polarity):\n",
    "    if polarity>0:\n",
    "        return \"positive\"\n",
    "    if polarity<0:\n",
    "        return \"negative\"\n",
    "    return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment']=tweets['polarity'].apply(lambda x:getSenti(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_csv(\"Tweets_Sentiment.csv\",index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string   \n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS=stopwords.words(\"english\")\n",
    "STOPWORDS.append(\"with\")\n",
    "ps = PorterStemmer() \n",
    "def clean_text(text):\n",
    "    ps=PorterStemmer()\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "    #remove extra white space\n",
    "    \n",
    "    text_cleaned=\"\".join([x for x in text if x not in string.punctuation])\n",
    "    \n",
    "    text_cleaned=re.sub(' +', ' ', text_cleaned)\n",
    "    text_cleaned=re.sub(r'[^\\x00-\\x7F]+',' ', text_cleaned)\n",
    "    text_cleaned=text_cleaned.lower()\n",
    "    tokens=text_cleaned.split(\" \")\n",
    "    tokens=[token for token in tokens if token not in STOPWORDS]\n",
    "    text_cleaned=\" \".join([ps.stem(token) for token in tokens])\n",
    "    \n",
    "    \n",
    "    return text_cleaned\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cleaned_tweet']=tweets['cleaned_tweet'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['usernameTweet', 'ID', 'text', 'url', 'nbr_retweet', 'nbr_favorite',\n",
       "       'nbr_reply', 'datetime', 'is_reply', 'is_retweet', 'user_id',\n",
       "       'has_media', 'medias', 'cleaned_tweet', 'score', 'polarity',\n",
       "       'subjectivity', 'sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(\"Tweets_Sentiment_v2.csv\",index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
